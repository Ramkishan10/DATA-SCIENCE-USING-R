# -*- coding: utf-8 -*-
"""Data_science_lab.r

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1upstKzx7r9wWg4Z_p6pXugoaNu_tXf-X

Ex-3-Reading and pre-processing the given data using R programming.
"""

library(dplyr)   # For data manipulation
library(readr)

# Load necessary libraries
install.packages("caret")  # Install caret package for pre-processing
library(caret)

data <- read.csv("/content/parkinsons.csv")

# Removing missing values
data <- na.omit(data)

# Removing duplicate rows
data <- distinct(data)

# Check for missing values
missing_values <- sum(is.na(data))
cat("Missing values before handling:", missing_values, "\n")

# Option 2: Impute missing values
# You can use different imputation methods such as mean, median, or k-nearest neighbors (knn)
# Example: Impute missing values with medianpreProcValues <- preProcess(data, method = c("medianImpute"))
preProcValues <- preProcess(data, method = c("medianImpute"))
data_imputed <- predict(preProcValues, newdata = data)

# Check for missing values after handling
missing_values_after <- sum(is.na(data_imputed))
cat("Missing values after handling:", missing_values_after, "\n")

head(data)

summary(data)

"""Ex-4-To understand the nature of data through box-plot analysis using R programming."""

# Install and load necessary libraries
install.packages("ggplot2")
library(ggplot2)

data <- read.csv("/content/parkinsons.csv")  # Replace "your_data_file.csv" with your actual file name

# Example: Create box plots for numerical variables
# Assuming your data contains numerical variables "MDVP.Fo.Hz.", "MDVP.Fhi.Hz.", and "MDVP.Flo.Hz."

# Create a list of numerical variables you want to analyze
numerical_vars <- c("MDVP.Fo.Hz.", "MDVP.Fhi.Hz.", "MDVP.Flo.Hz.")

# Create box plots for each numerical variable and display them
for (var in numerical_vars) {
  # Create box plot
  p <- ggplot(data, aes(x = "", y = .data[[var]])) +
    geom_boxplot(fill = "skyblue", color = "black") +
    labs(title = paste("Box Plot of", var),
         y = var) +
    theme_minimal()

  # Print the box plot
  print(p)
}

# Read the data into R
data <- read.csv("/content/parkinsons.csv")  # Replace "your_data_file.csv" with your actual file name

# Example: Create box plots for numerical variables
# Assuming your data contains numerical variables "MDVP.Fo.Hz.", "MDVP.Fhi.Hz.", and "MDVP.Flo.Hz."

# Convert data to long format
library(tidyr)
data_long <- gather(data, key = "Variable", value = "Value", -name)

# Create box plot
p <- ggplot(data_long, aes(x = Variable, y = Value)) +
  geom_boxplot(fill = "skyblue", color = "black") +
  labs(title = "Box Plot of Numerical Variables",
       x = "Variable",
       y = "Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # Rotate x-axis labels
  facet_wrap(~ Variable, scales = "free_y")  # Create separate panels for each variable

# Print the box plot
print(p)

# Read the data into R
data <- read.csv("/content/parkinsons.csv")  # Replace "your_data_file.csv" with your actual file name

# Example: Create box plots for numerical variables
# Assuming your data contains numerical variables "MDVP.Fo.Hz.", "MDVP.Fhi.Hz.", and "MDVP.Flo.Hz."

# Convert data to long format
library(tidyr)
data_long <- gather(data, key = "Variable", value = "Value", -name)

# Create box plot
p <- ggplot(data_long, aes(x = Variable, y = Value, fill = Variable)) +
  geom_boxplot(color = "black") +
  labs(title = "Box Plot of Numerical Variables",
       x = "Variable",
       y = "Value",
       fill = "Variable") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels

# Print the box plot
print(p)

"""Ex-5-Correlation analysis for feature selection using R programming"""

# Read the data into R
data <- read.csv("/content/parkinsons.csv")  # Replace "your_data_file.csv" with your actual file name

# Check data types
str(data)

# Convert non-numeric columns to numeric if appropriate
# For example, if you have categorical variables, you might encode them

# Subset numeric columns for correlation analysis
numeric_data <- data[, sapply(data, is.numeric)]

# Check if there are any missing values in the numeric data
if (anyNA(numeric_data)) {
  print("There are missing values in the numeric data. You may need to handle them before proceeding.")
} else {
  # Calculate the correlation matrix
  correlation_matrix <- cor(numeric_data)

  # Visualize the correlation matrix using a heatmap
  library(ggplot2)
  library(reshape2)

  # Melt the correlation matrix for visualization
  melted_correlation <- melt(correlation_matrix)

  # Create a heatmap of the correlation matrix with adjusted color and format style
  p <- ggplot(melted_correlation, aes(x = Var1, y = Var2, fill = value)) +
    geom_tile(color = "white") +
    scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1, 1), space = "Lab", name="Correlation") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          axis.text.y = element_text(angle = 45, hjust = 1),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank(),
          panel.border = element_rect(colour = "black", fill = NA),
          legend.position = "bottom") +
    labs(title = "Correlation Heatmap")

  # Print the heatmap
  print(p)

  # Perform feature selection based on correlation
  # Select variables with correlation above a certain threshold
  threshold <- 0.6  # Set your correlation threshold
  highly_correlated <- findCorrelation(correlation_matrix, cutoff = threshold)

  # Get the names of highly correlated variables
  highly_correlated_vars <- colnames(numeric_data)[highly_correlated]

  # Print the names of highly correlated variables
  print("Highly correlated variables:")
  print(highly_correlated_vars)
}

head(data)

"""pearson coefficients"""

# Install necessary R packages
install.packages("corrplot")

# Load the corrplot library
library(corrplot)

# Assuming 'correlation_matrix' is your correlation matrix

# Create a heatmap of the correlation matrix
corrplot(correlation_matrix, method = "color", type = "upper", order = "hclust",
         addrect = 8, rect.col = "black", tl.col = "black", tl.srt = 45)

"""Spearman"""

# Load necessary libraries
install.packages("corrplot")
library(corrplot)

# Assuming 'numerical_data' is your dataset containing numerical variables

# Calculate Spearman rank correlation coefficients
correlation_matrix_spearman <- cor(numerical_data, method = "spearman")

# Create a heatmap of the Spearman correlation matrix
corrplot(correlation_matrix_spearman, method = "color", type = "upper",
          order = "hclust", addrect = 8, rect.col = "black",
          tl.col = "black", tl.srt = 45)

"""Ex-6-Trend analysis through regression model using R programming"""

# Read the data into R
data <- read.csv("/content/parkinsons.csv")  # Replace "your_data_file.csv" with your actual file name

# Check the structure of the data
str(data)

# Perform trend analysis using linear regression
# Let's assume you want to analyze the trend between two numeric variables: "MDVP.Fo.Hz." and "MDVP.Fhi.Hz."

# Fit a linear regression model
model <- lm(MDVP.Fhi.Hz. ~ MDVP.Fo.Hz., data = data)

# Summary of the regression model
summary(model)

# Visualize the trend using a scatter plot with the regression line
library(ggplot2)

ggplot(data, aes(x = MDVP.Fo.Hz., y = MDVP.Fhi.Hz.)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +  # Add regression line
  labs(title = "Trend Analysis",
       x = "MDVP.Fo.Hz.",
       y = "MDVP.Fhi.Hz.")

"""Ex-7-Predictive analysis on health care data using R programming."""

# 1. Read the dataset from Google Colab
# Use appropriate file path and function to read the dataset
# For example:
data <- read.csv("/content/parkinsons.csv")

# 2. Preprocess the data (if needed)
# Perform any necessary preprocessing steps such as handling missing values,
# encoding categorical variables, or scaling numerical features

# 3. Select features and target variable
# Adjust this based on your dataset
# Adjust the column names to match the ones provided
X <- data[, c("NHR", "HNR", "status", "RPDE", "DFA", "spread1", "spread2", "D2", "PPE")]
# Include all relevant features
y <- data$status  # Assuming 'status' is the target variable

# 4. Fit the model
# Choose an appropriate model and fit it to the data
# For example, if you want to use logistic regression:
model <- glm(status ~ ., family = binomial(link = "logit"), data = data)

# 5. Make predictions
# Use the trained model to make predictions on the dataset
predictions <- predict(model, newdata = data, type = "response")

# 6. Visualize the predictions (optional)
# Depending on your dataset and problem, you can visualize the predictions
# For example, you can create a histogram of predicted probabilities or any other suitable plot

# For example:
hist(predictions, main = "Histogram of Predicted Probabilities", xlab = "Predicted Probability")

# Alternatively, you can use a scatter plot if you have actual labels to compare with predictions
# scatter plot of actual vs. predicted
plot(data$status, predictions, main = "Actual vs. Predicted", xlab = "Actual", ylab = "Predicted")

# You can also evaluate the model's performance using appropriate metrics if you have ground truth labels

# Note: Replace "/content/your_dataset.csv" with the actual path to your dataset in Google Colab

# Fit the model
model <- glm(status ~ ., family = binomial(link = "logit"), data = data)

# Predict the outcomes
predictions <- predict(model, type = "response")

# Create a scatter plot to visualize the actual vs. predicted outcomes
plot(data$status, predictions, col = ifelse(data$status == 1, "red", "blue"),
     main = "Actual vs. Predicted Outcomes", xlab = "Actual Outcome", ylab = "Predicted Outcome")

# Add a legend
legend("topright", legend = c("Parkinson's", "Healthy"), col = c("red", "blue"), pch = 1, cex = 0.8)

# Read the data
data <- read.csv("/content/parkinsons.csv")  # Replace "your_data_file.csv" with your actual file name

# Fit the logistic regression model
model <- glm(status ~ ., family = binomial(link = "logit"), data = data)

# Predict the outcomes
predictions <- predict(model, type = "response")

# Create a bar plot to visualize the actual vs. predicted outcomes
barplot(table(data$status, round(predictions)), beside = TRUE,
        col = c("red", "blue"), legend.text = c("Predicted Negative", "Predicted Positive"),
        main = "Actual vs. Predicted Outcomes", xlab = "Outcome", ylab = "Frequency")

# 1. Read the dataset from Google Colab
data <- read.csv("/content/parkinsons.csv")

# 2. Preprocess the data (if needed)
# Perform any necessary preprocessing steps such as handling missing values,
# encoding categorical variables, or scaling numerical features

# 3. Select features and target variable
X <- data[, c("NHR", "HNR", "status", "RPDE", "DFA", "spread1", "spread2", "D2", "PPE")]
y <- data$status

# 4. Fit the model
model <- glm(status ~ ., family = binomial(link = "logit"), data = data)

# 5. Make predictions
predictions <- predict(model, newdata = data, type = "response")

# 6. Visualize the predictions (optional)
# Histogram of predicted probabilities
hist(predictions, main = "Histogram of Predicted Probabilities", xlab = "Predicted Probability", col = "skyblue")

# Scatter plot of actual vs. predicted
plot(data$status, predictions, main = "Actual vs. Predicted", xlab = "Actual", ylab = "Predicted", col = ifelse(data$status == 1, "red", "blue"))
legend("topright", legend = c("Parkinson's", "Healthy"), col = c("red", "blue"), pch = 1, bty = "n")

colnames(data)

install.packages("glmnet", repos='http://cran.us.r-project.org')

# Load the necessary library
library(glmnet)

# Read the Parkinson's dataset from Google Colab
data <- read.csv("/content/parkinsons.csv")

# Select features and target variable
X <- data[, c("NHR", "HNR", "status", "RPDE", "DFA", "spread1", "spread2", "D2", "PPE")]
y <- data$status

# Fit the logistic regression model
model <- glm(status ~ ., family = binomial(link = "logit"), data = data)

# Generate predictions
predictions <- predict(model, newdata = data, type = "response")

# Create a scatter plot of actual vs. predicted values
plot(data$status, predictions,
     main = "Actual vs. Predicted",
     xlab = "Actual",
     ylab = "Predicted",
     col = "blue", # Change point color
     pch = 19)     # Change point shape

# Overlay a 45-degree line for reference
abline(a = 0, b = 1, col = "red")

# Add a legend
legend("bottomright",
       legend = c("Actual vs. Predicted", "45-degree Line"),
       col = c("blue", "red"),
       pch = c(19, NA))

# Evaluate the model's performance using appropriate metrics if available

"""EX.NO: 8              PRESCRIPTIVE ANALYSIS ON SALES DATA USING R-PROGRAMMING"""

install.packages("ggplot2")
library(ggplot2)

data <- read.csv("/content/supermarket_sales - Sheet1.csv", header = TRUE)

# Check the structure of the dataset
str(data)

# Summary statistics
summary(data)

# Plot histograms for numerical variables
ggplot(data, aes(x = Total, fill = Gender)) +
  geom_histogram(binwidth = 100, alpha = 0.7) +
  facet_wrap(~ Branch, nrow = 1) +
  labs(title = "Total Sales Distribution by Branch and Gender",
       x = "Total Sales", y = "Frequency")

colnames(data)

# Fit linear regression model
model <- lm(Total ~ Quantity + Unit.price + Tax.5. + cogs, data = data)

# Summary of the model
summary(model)

# Predict total sales
predictions <- predict(model, newdata = data)

# Plot actual vs. predicted sales
plot(data$Total, predictions, main = "Actual vs. Predicted Total Sales",
     xlab = "Actual Sales", ylab = "Predicted Sales")
abline(0, 1, col = "red")

# Calculate RMSE
rmse <- sqrt(mean((data$Total - predictions)^2))
cat("Root Mean Squared Error (RMSE):", rmse, "\n")

# Calculate MAE
mae <- mean(abs(data$Total - predictions))
cat("Mean Absolute Error (MAE):", mae, "\n")

# Calculate R-squared
rsquared <- summary(model)$r.squared
cat("R-squared:", rsquared, "\n")